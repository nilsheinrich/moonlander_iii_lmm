---
title: "Nils Wendel Heinrich: Distant Fixations"
subtitle: "Moonlander III - Analysis"
author: "Nils Wendel Heinrich"
date: "2024-06-17"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---
# Helpful
Shift + Control + ´ (right of ß) to close and open terminal...

# Description
...

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

# Modeling fixation duration

## Code book
Possible random effects: only **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment3_DistantFixations_short.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles])

# Filtering fixations with duration less than 25 samples
# fixdur >= 0.0125
my_data = my_data[(my_data.fixation_duration .>= 0.0125), :]
#eliminating fixations outside of game boarders
my_data = my_data[(my_data.distance_to_spaceship .< 16.63762484977781), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
#my_cake = Dict(
#  :ID => Grouping(),
#);

my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0 0 0
      0 -1 +1 0 0
      0 0 -1 +1 0
      0 0 0 -1 +1
    ];
    levels=[0.0, 0.5, 1.0, 1.5, 2.0],
    labels=["0.5", "1.0", "1.5", "2.0"],
  ),
);

```

# Modeling fixation duration

## Building various models

### Only varying intercept LMM

Varying intercept only for **ID**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparameterized
```

### Exploring random effects structure of the model
 We start by building the most complex random effects structure around ID (just dumping all of the fixed effects in the varying slope). 

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles + input_noise + (1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # Overparameterized
#VarCorr(m_varyingSlope_complex)
#last(m_varyingSlope_complex.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

Build the complex model but without correlations between random effects.
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles + input_noise + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # Overparameterized

```

Both full random slope structures are overparameterized. We have to kick individual random slopes.

### Building various models with varying slope of less complexity by throwing out random effects

leaving out input noise completely:
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles + input_noise + zerocorr(1 + N_visible_obstacles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)  # Not overparameterized
```
This one works out.

Leaving out N_visible_obstacles:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles + input_noise + zerocorr(1 + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)  # Not overparameterized

```
This one is singular again.

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
We will refer to m_varyingInt1 for hypothesis testing as it features a slightly smaller BIC.

## Model selection
```{julia}
#| label: selected model

m_varyingInt1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles + input_noise 
    + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

m_varyingInt1

```

## Principal component analysis

```{julia}

MixedModels.PCA(m_varyingInt1)

```

In the output, we will look at the normalized cumulative variances (second table). Each PC is focused individually. Do the loads make sense? For example, are loads high for visible obstacles and low for inputNoise?

## Caterpillar plot

We can visually check for having no correlation between random effects with a caterpillar plot.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingInt
#| label: fig-cm_varyingInt
#|
cm_varyingInt = first(ranefinfo(m_varyingInt1));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingInt; orderby=1)
```

## Shrinkage plot
This plot shows where strength was borrowed and applied to the data (in terms of adjusting linear trends).
```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
#shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope1)

```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:
```{julia}
confint(samples)
```

The ridgeplot will show us the estimates and their distributions (as plotted above). We will omit the intercept because it would zoom out too much.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Fixation duration (distant fixations)")
```

**Discussing the results:** 
  Fixed-effects parameters:
─────────────────────────────────────────────────────────────
                           Coef.  Std. Error      z  Pr(>|z|)
─────────────────────────────────────────────────────────────
(Intercept)          -1.94129     0.431296    -4.50    <1e-05
N_visible_obstacles   0.0251438   0.00290809   8.65    <1e-17
input_noise: 0.5      0.0447076   0.0283517    1.58    0.1148
input_noise: 1.0      0.00625462  0.0300117    0.21    0.8349
input_noise: 1.5      0.046539    0.0294153    1.58    0.1136
input_noise: 2.0     -0.0534997   0.0288743   -1.85    0.0639
─────────────────────────────────────────────────────────────

95% CIs:
β1  │ -2.77218    -1.07276
β2  │ 0.0195991   0.0307794
β3  │ -0.0106746  0.100982
β4  │ -0.0564663  0.0617446
β5  │ -0.0116303  0.103128
β6  │ -0.10848    0.00425392

**N_visible_obstacles** increases fixation duration in distant fixations (0.0251438, 0.00290809, [0.0195991, 0.0307794], p<.001).


# Modeling fixation location - distance to spaceship

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(1/sqrt(distance_to_spaceship) ~ 1 + N_visible_obstacles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(1/sqrt(distance_to_spaceship) ~ 1 + N_visible_obstacles + input_noise + (1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # Overparameterized

```

Stating zero correlation between random effects
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(1/sqrt(distance_to_spaceship) ~ 1 + N_visible_obstacles + input_noise 
    + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # Overparameterized

```

### Deleting individual random effects

Deleting input noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(1/sqrt(distance_to_spaceship) ~ 1 + N_visible_obstacles + input_noise 
    + zerocorr(1 + N_visible_obstacles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)

```
This one works out!

Kicking N_visible_obstacles out.
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(1/sqrt(distance_to_spaceship) ~ 1 + N_visible_obstacles + input_noise 
    + zerocorr(1 + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)
#VarCorr(m_varyingSlope2)

```
singular again.

Testing models
```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
We will refer to m_varyingInt1 for hypothesis testing.

## Model selection
```{julia}
#| label: selected model

m_varyingInt1 = let
    formula = @formula(1/sqrt(distance_to_spaceship) ~ 1 + N_visible_obstacles + input_noise 
    + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

m_varyingInt1

```
Now we can actually take a closer look at the main effects.

## Caterpillar plot

We can confirm the correlation between random effects visually by looking at the caterpillar:
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingInt = first(ranefinfo(m_varyingInt1));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingInt; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
#shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope1)

```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

Calling confint will give you the bounds, when visually it might be hard to verify significance of effects...
```{julia}
confint(samples)
```

Now let's plot the bounds (without intercept) to visualize when 0 is within the bounds (meaning no significance). It's basically the plot above for the beta estimates but every estimate gets its own row, which makes it easier to read.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to agent (distant fixations)")
```


  Fixed-effects parameters:
────────────────────────────────────────────────────────────────
                            Coef.   Std. Error       z  Pr(>|z|)
────────────────────────────────────────────────────────────────
(Intercept)           0.350244     0.0031555    110.99    <1e-99
N_visible_obstacles   0.000983074  0.000105281    9.34    <1e-20
input_noise: 0.5     -0.00238919   0.00102644    -2.33    0.0199
input_noise: 1.0     -0.00376153   0.00108656    -3.46    0.0005
input_noise: 1.5      0.00357887   0.00106488     3.36    0.0008
input_noise: 2.0      0.00104545   0.00104522     1.00    0.3172
────────────────────────────────────────────────────────────────

CI:
 β2  │ 0.000775249  0.00118684
 β3  │ -0.00437983  -0.0003907
 β4  │ -0.00585791  -0.00161187
 β5  │ 0.00149832   0.00565273
 β6  │ -0.00093964  0.00316557


# Modeling fixation location - distance to closest obstacle
Controlling for the number of visible obstacles.

```{julia}
#my_cake = Dict(
#  :ID => Grouping(),
#  :N_visible_obstacles => Grouping(),
#);


my_cake = Dict(
  :ID => Grouping(),
  :N_visible_obstacles => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0 0 0
      0 -1 +1 0 0
      0 0 -1 +1 0
      0 0 0 -1 +1
    ];
    levels=[0.0, 0.5, 1.0, 1.5, 2.0],
    labels=["0.5", "1.0", "1.5", "2.0"],
  ),
);
```

Varying intercepts for **ID** and **N_visible_obstacles**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + input_noise 
    + (1 | ID) 
    + (1 | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

VarCorr(m_varyingInt1)

```
We neglect ID as random intercept effect, but keep N_visible_obstacles.

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + input_noise 
    + (1 + input_noise | ID) 
    + (1 + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # Overparameterized

```

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + input_noise 
    + zerocorr(1 + input_noise | ID) 
    + zerocorr(1 + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # Also overparameterized

```

Referring to m_varyingInt1 for hypothesis testing:

## Model selection
```{julia}
#| label: selected model

m_varyingInt1 = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + input_noise
    + (1 | ID) 
    + (1 | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

m_varyingInt1

```

## Caterpillar plot
We can visually verify having stated zero correlation between random effects.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope_complex
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingInt1));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot
```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingInt1)

```
Some points were aggressively adjusted.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```
We already see that input noise does not significantly influences the distance to the closest obstacle in distant fixations.

### Plotting
Taking a look at the distribution of the estimates for the main effect:

```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to closest obstacle (distant fixations)")
```

Main effects for **input noise**
───────────────────────────────────────────────────────────
                         Coef.  Std. Error      z  Pr(>|z|)
───────────────────────────────────────────────────────────
(Intercept)        1.2005        0.0909016  13.21    <1e-39
input_noise: 0.5   0.0377768     0.0131105   2.88    0.0040
input_noise: 1.0   0.0111071     0.0138817   0.80    0.4236
input_noise: 1.5  -0.000894057   0.0135973  -0.07    0.9476
input_noise: 2.0   0.0178398     0.0133595   1.34    0.1818
───────────────────────────────────────────────────────────

CIs:
 β2  │ 0.0132766    0.0645856
 β3  │ -0.01521     0.0393782
 β4  │ -0.0265321   0.027214
 β5  │ -0.00795428  0.0441774

Sole effect for **input noise:05**
