---
title: "Nils Wendel Heinrich: SoC Responses"
subtitle: "Moonlander III - Analysis"
author: "Nils Wendel Heinrich"
date: "2024-08-06"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
Covariates (continuous variables):
    - N_prior_crashs
    - N_consecutive_crash_success
    (- trials_since_last_crash)
    (- crashed_in_last_trial)
    - N_fixations
Fixed Effects (categorical variables):
    - done
    - level_difficulty
    - drift
    - input noise

We will predict SoC judgement rating. Responses were given on a 7-step Likert scale. We will use parametric statistics assuming that the tests are sufficiently robust for this type of data.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

One possible random effect: **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment3_SoCData.arrow"))

# new variable: level_difficulty based on level
# 1 & 2: easy
# 3 & 4: medium
# 5 & 6: hard


describe(my_data)
```

### Contrasts

We will declare **ID** a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
We will approach defining contrasts for input noise in 2 possible ways. It may either be done by hypothesis coding with each level being compared to the level before, or it may simply be linear.
```{julia}

my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0 0 0
      0 -1 +1 0 0
      0 0 -1 +1 0
      0 0 0 -1 +1
    ];
    levels=[0.0, 0.5, 1.0, 1.5, 2.0],
    labels=["0.5", "1.0", "1.5", "2.0"],
  ),
);

my_cake_lin = Dict(
  :ID => Grouping(),
);

```

## Building various models

### Only varying intercept LMM

We will built a model for the custom constrast and the linear one and compare them via likelihood ratio test:

```{julia}

m_varyingInt1_hypo = let
    varInt = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

```

```{julia}

m_varyingInt1_lin = let
    varInt = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake_lin);
  end

```

#### Likelihood ratio test
```{julia}
#| label: lik ratio test

MixedModels.likelihoodratiotest(m_varyingInt1_hypo, m_varyingInt1_lin)

```
There is no significant difference between the models. But the one with the input noise being defined as linear is more complex featuring 3 more parameters. We will therefore use the custom contrast from here on...

```{julia}

VarCorr(m_varyingInt1_hypo)

```
The random intercept model hints towards ID being a valid random effect. Proceeding by including random slope effects.

### Most complex model
Simply dumping all fixed effect terms into the random effects structure.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
The model is too complex. Will start by stating zero correlation:

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # not overparamterized
```
This is the most complex model we can built. Starting from here...

Starting to delete single random slope effects while keeping zerocorr... 

Deleting done:
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # Overparamterized
```

Instead deleting input_noise:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done + level_difficulty + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2) # Overparamterized
```

```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3) # Overparamterized
```

Any combination does not work. What does though is any single random slope effect. We will thus build all the models with single random slopes and compare those against the only varying intercept model.

### single random slope models

```{julia}
#| label: m_varyingSlope_zc_ncs

m_varyingSlope_zc_ncs = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_ncs) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_zc_ncs]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_zc_ncs)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC=773.0 vs. 835.0. Staying with m_varyingSlope_complex_zc.

```{julia}
#| label: m_varyingSlope_zc_done

m_varyingSlope_zc_done = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_done) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_zc_done]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_zc_done)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Proceeding with m_varyingSlope_complex_zc.

```{julia}
#| label: m_varyingSlope_zc_ld

m_varyingSlope_zc_ld = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + level_difficulty | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_ld) # Overparamterized
```
This one will be excluded.

```{julia}
#| label: m_varyingSlope_zc_in

m_varyingSlope_zc_in = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_in) # overparamterized
```
Also singular...

```{julia}
#| label: m_varyingSlope_zc_nf

m_varyingSlope_zc_nf = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + N_fixations | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_nf) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_zc_nf]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_zc_nf)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Still m_varyingSlope_complex_zc is better.

m_varyingSlope_complex_zc has the smallest BIC. We will therefore do hypothesis testing on the basis of this model.

## Model selection
```{julia}
#| label: selected model

m_varyingSlope_complex_zc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

m_varyingSlope_complex_zc

```

## Bootstrapping
```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="SoC judgements")
```

**Discussing the results:** We find significant effects for (stating 95% CIs):
  Fixed-effects parameters:
─────────────────────────────────────────────────────────────────────
                                   Coef.  Std. Error      z  Pr(>|z|)
─────────────────────────────────────────────────────────────────────
(Intercept)                   3.94316      0.533372    7.39    <1e-12
done                          1.73504      0.297304    5.84    <1e-08
level_difficulty: medium     -0.0160268    0.101936   -0.16    0.8751
input_noise: 0.5             -0.241697     0.16803    -1.44    0.1503
input_noise: 1.0             -0.292297     0.244359   -1.20    0.2316
input_noise: 1.5             -0.897577     0.41257    -2.18    0.0296
input_noise: 2.0             -0.288491     0.210429   -1.37    0.1704
N_fixations                   0.00267252   0.0027515   0.97    0.3314
N_consecutive_crash_success  -0.0312364    0.0545581  -0.57    0.5670
─────────────────────────────────────────────────────────────────────

95% CIs:
 β1  │ 2.9074       4.99311
 β2  │ 1.14487      2.30549    # done
 β3  │ -0.209124    0.180006
 β4  │ -0.584629    0.0790631
 β5  │ -0.752272    0.204499
 β6  │ -1.71735     -0.109483
 β7  │ -0.707735    0.109423
 β8  │ -0.00311607  0.00819029
 β9  │ -0.143506    0.0716458

### Frome these findings, we will only report done and will further include this covariate as random effect.

# Including DONE as random effect because we are not interested in variance caused by success
random effects coding
```{julia}

my_cake = Dict(
  :ID => Grouping(),
  :done => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0 0 0
      0 -1 +1 0 0
      0 0 -1 +1 0
      0 0 0 -1 +1
    ];
    levels=[0.0, 0.5, 1.0, 1.5, 2.0],
    labels=["0.5", "1.0", "1.5", "2.0"],
  ),
);

```

only random intercept model
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID)
    + (1 | done));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

#issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
#last(m_varyingInt1.λ)

```
done is suitable for a random effect. Next up, exploring random slopes.

## Exploring random slope effects
```{julia}
#| label: m_varyingSlope_in

m_varyingSlope_in = let
    varSlop = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success
    + (1 + input_noise | ID)
    + (1 + input_noise | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_in) # Overparamterized

```
...

```{julia}
#| label: m_varyingSlope_in

m_varyingSlope_in = let
    varSlop = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success
    + (1 + input_noise | ID)
    + (1 | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_in) # Overparamterized

```
Introducing random slopes doesn't work. We will stick with the random intercept only model.

## Model selection
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID)
    + (1 | done));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

m_varyingInt1

```

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="SoC judgements")
```

**Discussing the results:** We find significant effects for (stating 95% CIs):

  Fixed-effects parameters:
───────────────────────────────────────────────────────────────────────
                                    Coef.   Std. Error      z  Pr(>|z|)
───────────────────────────────────────────────────────────────────────
(Intercept)                   4.87801      0.835042      5.84    <1e-08
level_difficulty: medium     -0.132197     0.142055     -0.93    0.3521
input_noise: 0.5             -0.187634     0.234644     -0.80    0.4239
input_noise: 1.0             -0.216892     0.233442     -0.93    0.3528
input_noise: 1.5             -0.887311     0.222914     -3.98    <1e-04
input_noise: 2.0             -0.253154     0.193334     -1.31    0.1904
N_fixations                   0.000568583  0.000859205   0.66    0.5081
N_consecutive_crash_success   0.00354867   0.02399       0.15    0.8824
───────────────────────────────────────────────────────────────────────

95% CIs:
 β1  │ 3.20519      6.50032
 β2  │ -0.417877    0.144874
 β3  │ -0.659938    0.26042
 β4  │ -0.676055    0.232458
 β5  │ -1.33324     -0.44727
 β6  │ -0.630719    0.121145
 β7  │ -0.00114322  0.0022469
 β8  │ -0.0416936   0.0517704
