---
title: "Nils Wendel Heinrich: SoC Responses"
subtitle: "Moonlander III - Analysis"
author: "Nils Wendel Heinrich"
date: "2024-08-06"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
Covariates (continuous variables):
    - N_prior_crashs
    - N_consecutive_crash_success
    (- trials_since_last_crash)
    (- crashed_in_last_trial)
    - N_fixations
Fixed Effects (categorical variables):
    - done
    - level_difficulty
    - drift
    - input noise

We will predict SoC judgement rating. Responses were given on a 7-step Likert scale. We will use parametric statistics assuming that the tests are sufficiently robust for this type of data.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

One possible random effect: **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment3_SoCData.arrow"))

# new variable: level_difficulty based on level
# 1 & 2: easy
# 3 & 4: medium
# 5 & 6: hard


describe(my_data)
```

### Contrasts

We will declare **ID** a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}

my_cake = Dict(
  :ID => Grouping(),
);

```

## Building various models

### Only varying intercept LMM

Already starting with N_consecutive_crash_success...

Varying intercepts for **ID**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
#VarCorr(m_varyingInt1)
#last(m_varyingInt1.λ)

```
The random intercept model hints towards ID being a valid random effect. Proceeding by including random slope effects.

### Most complex model
Simply dumping all fixed effect terms into the random effects structure.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
The model is too complex. Will start by stating zero correlation:

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # overparamterized
```
Still detecting singularity.

Starting to delete single random slope effects while keeping zerocorr... Any combination does not work. What does though is any single random slope effect. We will thus build all the models with single random slopes and compare those against the only varying intercept model.

### single random slope models

```{julia}
#| label: m_varyingSlope_zc_ncs

m_varyingSlope_zc_ncs = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_ncs) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_zc_ncs]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_zc_ncs)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC=821.

```{julia}
#| label: m_varyingSlope_zc_done

m_varyingSlope_zc_done = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + done | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_done) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_zc_done]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_zc_done)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC=798.

```{julia}
#| label: m_varyingSlope_zc_ld

m_varyingSlope_zc_ld = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + level_difficulty | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_ld) # Overparamterized
```
This one will be excluded.

```{julia}
#| label: m_varyingSlope_zc_in

m_varyingSlope_zc_in = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_in) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_zc_in]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_zc_in)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC=792.

```{julia}
#| label: m_varyingSlope_zc_nf

m_varyingSlope_zc_nf = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + N_fixations | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_nf) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_zc_nf]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_zc_nf)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC=813.

m_varyingSlope_zc_in has the smallest BIC. We will therefore do hypothesis testing on the basis of this model.

## Model selection
```{julia}
#| label: m_varyingSlope_zc_in

m_varyingSlope_zc_in = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + zerocorr(1 + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

```

## Bootstrapping
```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_zc_in)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="SoC judgements")
```

**Discussing the results:** We find significant effects for (stating 95% CIs):
- done (1.70564, 2.33478)
- input_noise (-1.4097, -0.325682)

### Frome these findings, we will only report done and will further include this covariate as random effect.

# Including DONE as random effect because we are not interested in variance caused by success
random effects coding
```{julia}

my_cake = Dict(
  :ID => Grouping(),
  :done => Grouping()
);

```

only random intercept model
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID)
    + (1 | done));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
#VarCorr(m_varyingInt1)
#last(m_varyingInt1.λ)

```
done is suitable for a random effect. Next up, exploring random slopes.

## Exploring random slope effects
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlop = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success
    + (1 | ID)
    + (1 | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # Not overparamterized

```
...
## Model selection
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + level_difficulty + input_noise + N_fixations + N_consecutive_crash_success 
    + (1 | ID)
    + (1 | done));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

```

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="SoC judgements")
```

**Discussing the results:** We find significant effects for (stating 95% CIs):

### with N_fixations:
- input_noise (-1.05016, -0.637036)
